{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 14\n",
    "\n",
    "Name:  Paul Martin Enano\n",
    "UID: U75020763\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Naive Bayes\n",
    "- Model Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "| Attribute A | Attribute B | Attribute C | Class |\n",
    "|-------------|-------------|-------------|-------|\n",
    "| Yes         | Single      | High        | No    |\n",
    "| No          | Married     | Mid         | No    |\n",
    "| No          | Single      | Low         | No    |\n",
    "| Yes         | Married     | High        | No    |\n",
    "| No          | Divorced    | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| Yes         | Divorced    | High        | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "\n",
    "a) Compute the following probabilities:\n",
    "\n",
    "- P(Attribute A = Yes | Class = No)\n",
    "- P(Attribute B = Divorced | Class = Yes)\n",
    "- P(Attribute C = High | Class = No)\n",
    "- P(Attribute C = Mid | Class = Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/81/3_yv30h56l90szxy_4kqtdgh0000gn/T/ipykernel_1439/1745997661.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42857142857142855, 0.3333333333333333, 0.42857142857142855, 1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Attribute A': ['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No'],\n",
    "    'Attribute B': ['Single', 'Married', 'Single', 'Married', 'Divorced', 'Married', 'Divorced', 'Single', 'Married', 'Single'],\n",
    "    'Attribute C': ['High', 'Mid', 'Low', 'High', 'Mid', 'Low', 'High', 'Mid', 'Low', 'Mid'],\n",
    "    'Class': ['No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "YN = len(df[(df['Attribute A'] == 'Yes') & (df['Class'] == 'No')]) / len(df[df['Class'] == 'No'])\n",
    "\n",
    "DY = len(df[(df['Attribute B'] == 'Divorced') & (df['Class'] == 'Yes')]) / len(df[df['Class'] == 'Yes'])\n",
    "\n",
    "HN = len(df[(df['Attribute C'] == 'High') & (df['Class'] == 'No')]) / len(df[df['Class'] == 'No'])\n",
    "\n",
    "MY = len(df[(df['Attribute C'] == 'Mid') & (df['Class'] == 'Yes')]) / len(df[df['Class'] == 'Yes'])\n",
    "\n",
    "YN, DY, HN, MY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.42, 0.33, 0.42, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Classify the following unseen records:\n",
    "\n",
    "- (Yes, Married, Mid)\n",
    "- (No, Divorced, High)\n",
    "- (No, Single, High)\n",
    "- (No, Divorced, Low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-No\n",
    "-No\n",
    "-No\n",
    "-No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "a) Write a function to generate the confusion matrix for a list of actual classes and a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AYPY': 2, 'ANPN': 4, 'ANPY': 3, 'AYPN': 1}\n"
     ]
    }
   ],
   "source": [
    "actualclass = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predictedclass = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "def confusionmatrix(actual, predicted):\n",
    "    AYPY = ANPN = ANPY = AYPN = 0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == 'Yes' and p == 'Yes':\n",
    "            AYPY += 1\n",
    "        elif a == 'No' and p == 'No':\n",
    "            ANPN += 1\n",
    "        elif a == 'No' and p == 'Yes':\n",
    "            ANPY += 1\n",
    "        elif a == 'Yes' and p == 'No':\n",
    "            AYPN += 1\n",
    "    return {\"AYPY\": AYPY, \"ANPN\": ANPN, \"ANPY\": ANPY, \"AYPN\": AYPN}\n",
    "\n",
    "print(confusionmatrix(actualclass, predictedclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume you have the following Cost Matrix:\n",
    "\n",
    "|            | predicted = Y | predicted = N |\n",
    "|------------|---------------|---------------|\n",
    "| actual = Y |       -1      |       5       |\n",
    "| actual = N |        10     |       0       |\n",
    "\n",
    "What is the cost of the above classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost of the above classification is 33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that takes in the actual values, the predictions, and a cost matrix and outputs a cost. Test it on the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "def classificationcost(actual, predicted, costmatrix):\n",
    "    AYPY = ANPN = ANPY = AYPN = 0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == 'Yes' and p == 'Yes':\n",
    "            AYPY += 1\n",
    "        elif a == 'No' and p == 'No':\n",
    "            ANPN += 1\n",
    "        elif a == 'No' and p == 'Yes':\n",
    "            ANPY += 1\n",
    "        elif a == 'Yes' and p == 'No':\n",
    "            AYPN += 1\n",
    "\n",
    "    totalcost = AYPY * costmatrix['AYPY'] + ANPN * costmatrix['ANPN'] + \\\n",
    "                 ANPY * costmatrix['ANPY'] + AYPN * costmatrix['AYPN']\n",
    "\n",
    "    return totalcost\n",
    "\n",
    "actualvalues = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predictedvalues = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "costmatrix = {\"AYPY\": -1, \"ANPN\": 0, \"ANPY\": 10, \"AYPN\": 5}\n",
    "\n",
    "cost = classificationcost(actualvalues, predictedvalues, costmatrix)\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Implement functions for the following:\n",
    "\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f-measure\n",
    "\n",
    "and apply them to the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.4, 0.6666666666666666, 0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getaccuracy(matr):\n",
    "    totalcorrect = matr['AYPY'] + matr['ANPN']\n",
    "    totalpopulation = sum(matr.values())\n",
    "    return totalcorrect / totalpopulation\n",
    "\n",
    "def getprecision(matr):\n",
    "    return matr['AYPY'] / (matr['AYPY'] + matr['ANPY'])\n",
    "\n",
    "def getrecall(matr):\n",
    "    return matr['AYPY'] / (matr['AYPY'] + matr['AYPN'])\n",
    "\n",
    "def getfmeasure(matr):\n",
    "    precision = getprecision(matr)\n",
    "    recall = getrecall(matr)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "example = {'AYPY': 2, 'ANPN': 4, 'ANPY': 3, 'AYPN': 1}\n",
    "\n",
    "accuracy = getaccuracy(example)\n",
    "precision = getprecision(example)\n",
    "recall = getrecall(example)\n",
    "fmeasure = getfmeasure(example)\n",
    "\n",
    "accuracy, precision, recall, fmeasure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge (Midterm prep part 2)\n",
    "\n",
    "In this exercise you will update your submission to the titanic competition.\n",
    "\n",
    "a) First let's add new numerical features / columns to the datasets that might be related to the survival of individuals.\n",
    "\n",
    "- `has_cabin` should have a value of 0 if the `cabin` feature is `nan` and 1 otherwise\n",
    "- `family_members` should have the total number of family members (by combining `SibSp` and `Parch`)\n",
    "- `title_type`: from the title extracted from the name, we will categorize it into 2 types: `common` for titles that many passengers have, `rare` for titles that few passengers have. Map `common` to 1 and `rare` to 0. Describe what threshold you used to define `common` and `rare` titles and how you found it.\n",
    "- `fare_type`: using Kmeans clustering on the fare column, find an appropriate number of clusters / groups of similar fares. Using the clusters you created, `fare_price` should be an ordinal variable that represents the expensiveness of the fare. For example if you split fare into 3 clusters ( 0 - 15, 15 - 40, and 40+ ) then the `fare_price` value should be `0` for `fare` values 0 - 15, `1` for 15 - 40, and `2` for 40+.\n",
    "- Create an addition two numerical features of your invention that you think could be relevant to the survival of individuals.\n",
    "\n",
    "Note: The features must be numerical because the sklearn `DecisionTreeClassifier` can only take on numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using a method covered in class, tune the parameters of a decision tree model on the titanic dataset (containing all numerical features including the ones you added above). Evaluate this model locally and report it's performance.\n",
    "\n",
    "Note: make sure you are not tuning your parameters on the same dataset you are using to evaluate the model. Also explain how you know you are not overfitting to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Try reducing the dimension of the dataset and create a Naive Bayes model. Evaluate this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Create an ensemble classifier using a combination of KNN, Decision Trees, and Naive Bayes models. Evaluate this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Update your kaggle submission using the best model you created (best model means the one that performed the best on your local evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful code for the midterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'makepipeline' from 'sklearn.pipeline' (/Users/enano/miniconda3/lib/python3.11/site-packages/sklearn/pipeline.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m makepipeline\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusionmatrix, accuracyscore\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetchlfwpeople\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'makepipeline' from 'sklearn.pipeline' (/Users/enano/miniconda3/lib/python3.11/site-packages/sklearn/pipeline.py)"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import makepipeline\n",
    "from sklearn.metrics import confusionmatrix, accuracyscore\n",
    "from sklearn.datasets import fetchlfwpeople\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.modelselection import GridSearchCV, traintestsplit\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Get face data\n",
    "faces = fetchlfwpeople(minfacesperperson=60)\n",
    "\n",
    "# plot face data\n",
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.targetnames[faces.target[i]])\n",
    "plt.show()\n",
    "\n",
    "# split train test set\n",
    "Xtrain, Xtest, ytrain, ytest = traintestsplit(faces.data, faces.target, randomstate=42)\n",
    "\n",
    "pca = PCA(ncomponents=150, whiten=True)\n",
    "svc = SVC(kernel='rbf', classweight='balanced')\n",
    "svcpca = makepipeline(pca, svc)\n",
    "\n",
    "# Tune model to find best values of C and gamma using cross validation\n",
    "paramgrid = {'svcC': [1, 5, 10, 50],\n",
    "              'svcgamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "kfold = 10\n",
    "grid = GridSearchCV(svcpca, paramgrid, cv=kfold)\n",
    "grid.fit(Xtrain, ytrain)\n",
    "\n",
    "print(grid.bestparams)\n",
    "\n",
    "# use the best params explicitly here\n",
    "pca = PCA(ncomponents=150, whiten=True)\n",
    "svc = SVC(kernel='rbf', classweight='balanced', C=10, gamma=0.005)\n",
    "svcpca = makepipeline(pca, svc)\n",
    "\n",
    "model = BaggingClassifier(svcpca, nestimators=100).fit(Xtrain, ytrain)\n",
    "yfit = model.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.setylabel(faces.targetnames[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)\n",
    "plt.show()\n",
    "\n",
    "mat = confusionmatrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.targetnames,\n",
    "            yticklabels=faces.targetnames)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy = \", accuracyscore(ytest, yfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
